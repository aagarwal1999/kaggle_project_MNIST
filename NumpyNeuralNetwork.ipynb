{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.io as spio\n",
    "k = spio.loadmat('Data/matlab/emnist-mnist.mat') #change 'letters' to 'mnist', 'balanced', 'byclass', 'bymerge' or 'digits' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = np.matrix(k['dataset'][0][0][0][0][0][0])\n",
    "lengt = len(training_data)\n",
    "labels = k['dataset'][0][0][0][0][0][1]\n",
    "\n",
    "size = len(np.unique(labels))\n",
    "print(np.unique(labels))\n",
    "labels = np.matrix(labels)\n",
    "testing_data = np.matrix(k['dataset'][0][0][1][0][0][0])\n",
    "testing_data_labels = np.matrix(k['dataset'][0][0][1][0][0][1])\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00881111418267\n",
      "0.0948995735616\n",
      "[[-0.08279207]\n",
      " [ 0.06676678]\n",
      " [-0.02490754]\n",
      " [-0.05737897]\n",
      " [ 0.03488756]\n",
      " [-0.01758978]\n",
      " [-0.01586535]\n",
      " [-0.02228512]\n",
      " [-0.08589676]\n",
      " [-0.07806628]]\n",
      "[[ 0.05004317  0.04064887 -0.0032867  ..., -0.0887481   0.02341749\n",
      "   0.08924996]\n",
      " [ 0.09373914 -0.06829579  0.00543383 ...,  0.03137334  0.00900837\n",
      "  -0.00128293]\n",
      " [-0.02738654  0.06182885 -0.08081667 ...,  0.02935118 -0.07888732\n",
      "   0.09391152]\n",
      " ..., \n",
      " [-0.00608862  0.06261471  0.06204737 ..., -0.07424761  0.00624031\n",
      "   0.03317486]\n",
      " [ 0.06180467 -0.06012533  0.04403709 ..., -0.00315576  0.06620123\n",
      "  -0.08300261]\n",
      " [ 0.0172702  -0.06337977  0.08554012 ...,  0.05260447  0.0698782\n",
      "  -0.04385505]]\n",
      "[[-0.00202598 -0.00409048  0.00802057 ...,  0.00331753 -0.00303903\n",
      "   0.00084142]\n",
      " [-0.0054347  -0.00243663  0.00403735 ...,  0.0051274  -0.0040617\n",
      "   0.00149533]\n",
      " [-0.00295054 -0.00179922  0.0062243  ..., -0.00208246 -0.0031559\n",
      "   0.00525953]\n",
      " ..., \n",
      " [ 0.00360665  0.0076611   0.0036361  ...,  0.00036395 -0.00132692\n",
      "  -0.00444459]\n",
      " [-0.00059024 -0.00553861 -0.00329732 ..., -0.00019272  0.00324406\n",
      "   0.00040043]\n",
      " [-0.0033037  -0.00790754 -0.00053297 ..., -0.00836976  0.00518972\n",
      "  -0.00519029]]\n",
      "[[ 0.00011338]\n",
      " [ 0.00352448]\n",
      " [-0.00343265]\n",
      " [ 0.00267347]\n",
      " [-0.00523984]\n",
      " [-0.00409662]\n",
      " [-0.00631044]\n",
      " [ 0.00657754]\n",
      " [ 0.00547449]\n",
      " [ 0.00358551]\n",
      " [ 0.00310836]\n",
      " [ 0.00705247]\n",
      " [-0.00649045]\n",
      " [ 0.00338062]\n",
      " [ 0.0044388 ]\n",
      " [-0.00112683]\n",
      " [ 0.0064881 ]\n",
      " [-0.00489176]\n",
      " [-0.00215513]\n",
      " [-0.0026557 ]\n",
      " [ 0.00212407]\n",
      " [ 0.00461649]\n",
      " [-0.00538803]\n",
      " [ 0.00831036]\n",
      " [-0.0072904 ]\n",
      " [-0.00164351]\n",
      " [-0.00468735]\n",
      " [-0.00113473]\n",
      " [ 0.00471491]\n",
      " [ 0.00571398]\n",
      " [-0.00076574]\n",
      " [ 0.00539092]\n",
      " [-0.00834408]\n",
      " [-0.00227107]\n",
      " [ 0.00276752]\n",
      " [-0.00251075]\n",
      " [ 0.00449792]\n",
      " [ 0.00093856]\n",
      " [-0.0044476 ]\n",
      " [ 0.00143502]\n",
      " [ 0.00719567]\n",
      " [-0.0069982 ]\n",
      " [ 0.00808444]\n",
      " [-0.00679349]\n",
      " [-0.00616784]\n",
      " [-0.00075334]\n",
      " [-0.0061686 ]\n",
      " [-0.00138236]\n",
      " [-0.00530431]\n",
      " [ 0.00241403]\n",
      " [-0.00811971]\n",
      " [ 0.00739319]\n",
      " [ 0.00029019]\n",
      " [-0.00648898]\n",
      " [-0.00548539]\n",
      " [-0.00551007]\n",
      " [ 0.00287714]\n",
      " [-0.00179099]\n",
      " [-0.00152729]\n",
      " [ 0.00263963]\n",
      " [-0.00268527]\n",
      " [-0.00310972]\n",
      " [ 0.00761116]\n",
      " [-0.00012135]\n",
      " [-0.00033005]\n",
      " [-0.0060848 ]\n",
      " [ 0.00092365]\n",
      " [ 0.00096632]\n",
      " [-0.00647642]\n",
      " [-0.00238705]\n",
      " [ 0.0042687 ]\n",
      " [ 0.00126119]\n",
      " [-0.00532511]\n",
      " [ 0.00329209]\n",
      " [ 0.00370081]\n",
      " [ 0.00400571]\n",
      " [ 0.00695301]\n",
      " [-0.00663712]\n",
      " [ 0.00048642]\n",
      " [ 0.00115435]\n",
      " [-0.00712088]\n",
      " [ 0.00243374]\n",
      " [-0.00732488]\n",
      " [ 0.0005563 ]\n",
      " [-0.00780569]\n",
      " [ 0.00186802]\n",
      " [ 0.0004762 ]\n",
      " [ 0.00546628]\n",
      " [-0.00418968]\n",
      " [-0.00625305]\n",
      " [-0.00603152]\n",
      " [ 0.00339792]\n",
      " [-0.00408745]\n",
      " [-0.00416283]\n",
      " [-0.0019512 ]\n",
      " [ 0.00064825]\n",
      " [ 0.0001261 ]\n",
      " [-0.00053942]\n",
      " [ 0.00795384]\n",
      " [-0.00514065]\n",
      " [-0.00104623]\n",
      " [ 0.0063983 ]\n",
      " [ 0.00510857]\n",
      " [ 0.00168942]\n",
      " [-0.0017475 ]\n",
      " [ 0.00821451]\n",
      " [ 0.00777264]\n",
      " [-0.00375116]\n",
      " [ 0.00662443]\n",
      " [ 0.00674198]\n",
      " [ 0.00259381]\n",
      " [ 0.00211538]\n",
      " [ 0.00025652]\n",
      " [ 0.00351198]\n",
      " [ 0.00680102]\n",
      " [-0.00060038]\n",
      " [-0.00456287]\n",
      " [ 0.00617018]\n",
      " [ 0.00578117]\n",
      " [-0.0022074 ]\n",
      " [-0.00133447]\n",
      " [-0.00295849]\n",
      " [-0.00305793]\n",
      " [ 0.00096751]\n",
      " [ 0.00756015]\n",
      " [ 0.00121728]\n",
      " [-0.00170016]\n",
      " [-0.00347368]\n",
      " [-0.00319204]\n",
      " [ 0.00218883]\n",
      " [ 0.00153206]\n",
      " [ 0.00532461]\n",
      " [ 0.00105582]\n",
      " [-0.0086731 ]\n",
      " [-0.00333335]\n",
      " [ 0.00875959]\n",
      " [ 0.00542829]\n",
      " [ 0.00025673]\n",
      " [-0.00287283]\n",
      " [ 0.00838918]\n",
      " [ 0.0018958 ]\n",
      " [ 0.00233277]\n",
      " [ 0.00830232]\n",
      " [-0.0019051 ]\n",
      " [ 0.00767898]\n",
      " [ 0.00831383]\n",
      " [-0.0069218 ]\n",
      " [ 0.00544691]\n",
      " [ 0.0014531 ]\n",
      " [-0.0047345 ]\n",
      " [-0.00743176]\n",
      " [ 0.00455428]\n",
      " [-0.00032203]\n",
      " [ 0.0015628 ]\n",
      " [-0.00670879]\n",
      " [-0.0003934 ]\n",
      " [ 0.0024386 ]\n",
      " [-0.0055329 ]\n",
      " [ 0.00599171]\n",
      " [ 0.00778964]\n",
      " [-0.00482407]\n",
      " [-0.00392667]\n",
      " [ 0.0038905 ]\n",
      " [ 0.00573249]\n",
      " [-0.00813815]\n",
      " [ 0.00329002]\n",
      " [ 0.00420309]\n",
      " [-0.00441114]\n",
      " [-0.00622744]\n",
      " [-0.00820937]\n",
      " [-0.00016778]\n",
      " [ 0.00470855]\n",
      " [-0.00354548]\n",
      " [-0.00351387]\n",
      " [ 0.00192653]\n",
      " [-0.0083375 ]\n",
      " [ 0.00017987]\n",
      " [ 0.00711094]\n",
      " [-0.00022133]\n",
      " [-0.00355925]\n",
      " [-0.00070464]\n",
      " [ 0.00059774]\n",
      " [-0.00430304]\n",
      " [ 0.00400051]\n",
      " [ 0.00547897]\n",
      " [-0.0080189 ]\n",
      " [ 0.00421456]\n",
      " [-0.00340524]\n",
      " [ 0.00101439]\n",
      " [-0.00613268]\n",
      " [-0.00620509]\n",
      " [-0.00268657]\n",
      " [ 0.00068155]\n",
      " [ 0.00506982]\n",
      " [ 0.00616199]\n",
      " [ 0.00839829]\n",
      " [ 0.0060501 ]\n",
      " [-0.00630743]\n",
      " [-0.00182396]\n",
      " [-0.00251714]\n",
      " [-0.00388851]\n",
      " [ 0.00834954]\n",
      " [ 0.00585681]\n",
      " [-0.00875859]\n",
      " [-0.0061344 ]\n",
      " [ 0.00220591]\n",
      " [-0.00527858]\n",
      " [-0.00219975]\n",
      " [-0.00474982]\n",
      " [ 0.00550268]\n",
      " [ 0.0070824 ]\n",
      " [ 0.0028333 ]\n",
      " [ 0.00786202]\n",
      " [-0.00368121]\n",
      " [ 0.0012348 ]\n",
      " [ 0.00772311]\n",
      " [ 0.00185734]\n",
      " [-0.00136703]\n",
      " [-0.00272421]\n",
      " [ 0.00423138]\n",
      " [ 0.00534201]\n",
      " [-0.00710543]\n",
      " [ 0.00868791]\n",
      " [-0.00853893]\n",
      " [-0.0075706 ]\n",
      " [ 0.00762621]\n",
      " [ 0.00569233]\n",
      " [-0.00664041]\n",
      " [ 0.00548145]\n",
      " [-0.00722205]\n",
      " [ 0.00120056]\n",
      " [ 0.00429392]\n",
      " [ 0.0033356 ]\n",
      " [ 0.00036012]\n",
      " [ 0.00583619]\n",
      " [ 0.0010436 ]\n",
      " [-0.00104469]\n",
      " [-0.00421649]\n",
      " [ 0.00375004]\n",
      " [-0.00159279]\n",
      " [-0.00616675]\n",
      " [-0.00676891]\n",
      " [ 0.005628  ]\n",
      " [-0.00702719]\n",
      " [-0.00779035]\n",
      " [ 0.00136387]\n",
      " [-0.00485291]\n",
      " [-0.00233894]\n",
      " [-0.00374958]\n",
      " [ 0.00227175]]\n",
      "(250, 1)\n"
     ]
    }
   ],
   "source": [
    "input_layer_size = training_data.shape[1]\n",
    "output_layer_size = size\n",
    "hidden_layer_size = 250\n",
    "learning_rate = 0.05\n",
    "e_init_lay1 = np.sqrt(6) / ((np.sqrt(input_layer_size)) + hidden_layer_size)\n",
    "e_init_lay2 = np.sqrt(6)/ (np.sqrt(hidden_layer_size) + output_layer_size)\n",
    "print(e_init_lay1)\n",
    "print(e_init_lay2)\n",
    "\n",
    "input_to_hidden_weight_matrix = np.matrix(2 * e_init_lay1 * np.random.rand(hidden_layer_size, input_layer_size)) - e_init_lay1\n",
    "input_to_hidden_bias_vector = np.matrix(2 * e_init_lay1 * np.random.rand(hidden_layer_size)).T - e_init_lay1\n",
    "hidden_to_output_weight_matrix = np.matrix(2 * e_init_lay2 * np.random.rand(output_layer_size, hidden_layer_size)) - e_init_lay2\n",
    "hidden_to_output_bias_vector = np.matrix(2 * e_init_lay2 * np.random.rand(output_layer_size)).T - e_init_lay2\n",
    "\n",
    "print(hidden_to_output_bias_vector)\n",
    "print(hidden_to_output_weight_matrix)\n",
    "print(input_to_hidden_weight_matrix)\n",
    "print(input_to_hidden_bias_vector)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def change_labels_to_actual(labels, size):\n",
    "    return_val = np.matlib.zeros((labels.shape[0], size))\n",
    "    for i in range(labels.shape[0]):\n",
    "        return_val[i, labels[i, 0]] = 1\n",
    "    return return_val\n",
    "labelp = change_labels_to_actual(labels, size)\n",
    "testing_data_label = change_labels_to_actual(testing_data_labels, size)\n",
    "labelp[0, :].shape\n",
    "print((input_to_hidden_weight_matrix * training_data[0, :].T).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          1.38629436]\n",
      " [ 0.69314718  1.60943791]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(npmat):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-1 * npmat))\n",
    "\n",
    "def apply_func_to_matrix(s, func):\n",
    "    for i in range(s.shape[0]):\n",
    "        for j in range(s.shape[1]):\n",
    "            s[i, j] = func(s[i, j])\n",
    "    return s\n",
    "\n",
    "def relu(npmat):\n",
    "    return 0.5 * abs(npmat) + .5* npmat\n",
    "\n",
    "def flatten(one, two, three, four):\n",
    "    o = np.array(np.ndarray.flatten(one))[0]\n",
    "    t = np.array(np.ndarray.flatten(two))[0]\n",
    "    th = np.array(np.ndarray.flatten(three))[0]\n",
    "    fo = np.array(np.ndarray.flatten(four))[0]\n",
    "    ot = np.append(o, t)\n",
    "    otth = np.append(ot, th)\n",
    "    otthfo = np.append(otth, fo)\n",
    "    return otthfo\n",
    "\n",
    "def unflatten(thet):\n",
    "    intohid = np.matrix(thet[:input_layer_size * hidden_layer_size]).reshape(hidden_layer_size, input_layer_size)\n",
    "    thet = thet[input_layer_size * hidden_layer_size:]\n",
    "    inbias = np.matrix(thet[:hidden_layer_size])\n",
    "    thet = thet[hidden_layer_size:]\n",
    "    hittoout = np.matrix(thet[:hidden_layer_size * output_layer_size].reshape(output_layer_size, hidden_layer_size))\n",
    "    thet = np.matrix(thet[hidden_layer_size * output_layer_size:])\n",
    "    return intohid, inbias.T, hittoout, thet.T\n",
    "\n",
    "print(np.log(np.matrix([[1, 4], [2, 5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predict_layer1(input_layer):\n",
    "    lay1 = input_to_hidden_weight_matrix * input_layer + input_to_hidden_bias_vector * np.matrix(np.ones(input_layer.shape[1]))\n",
    "    return lay1\n",
    "def compute_activation_layer(activation, layer):\n",
    "    return activation(layer)\n",
    "def compute_predict_layer2(lay1):\n",
    "    lay2 = hidden_to_output_weight_matrix * lay1 + hidden_to_output_bias_vector * np.matrix(np.ones(lay1.shape[1]))\n",
    "    return lay2\n",
    "\n",
    "def compute_cost_cross_entropy(true_values, predicted_values):\n",
    "    sum = 0\n",
    "    \n",
    "    for k in range(true_values.shape[0]):\n",
    "        \n",
    "       \n",
    "        sum+= np.log(predicted_values[k, 0]) * true_values[k, 0] + (1 - true_values[k, 0]) * np.log(1 - predicted_values[k, 0])\n",
    "        \n",
    "    \n",
    "    return -1 * sum\n",
    "def compute_cost_mean_squared_error(true_values, predicted_values):\n",
    "    return np.matrix(np.apply_along_axis(sum_of_squares, 1, np.array(true_values - predicted_values)))\n",
    "    \n",
    "def sum_of_squares(arr):\n",
    "    return sum([x*x for x in arr])\n",
    "\n",
    "def gradient_mean_squared_error(true_values, predicted_values):\n",
    "    return 2* (true_values - predicted_values)\n",
    "\n",
    "\n",
    "\n",
    "def gradient_cross_entropy(true_values, activation_three):\n",
    "    return  activation_three - true_values\n",
    "\n",
    "def gradient_bias_layer1(delta_3, activation_gradient_of_z):\n",
    "    \n",
    "    return np.multiply(hidden_to_output_weight_matrix.T * delta_3, activation_gradient_of_z)\n",
    "\n",
    "def relu_gradient(layer):\n",
    "    def relu_grad_one(num):\n",
    "        if num > 0:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    return apply_func_to_matrix(layer, relu_grad_one)\n",
    "\n",
    "def sigmoid_gradient(layer):\n",
    "    def sigmoid_grad_one(num):\n",
    "        return (1 - sigmoid(num)) * sigmoid(num)\n",
    "    return apply_func_to_matrix(layer, sigmoid_grad_one)\n",
    "def compute_gradient_bias_layer2(predicted_values, activation_three, grad_function):\n",
    "    return grad_function(predicted_values, activation_three)\n",
    "def compute_gradient_hidden_to_output(delta_3, a_2):\n",
    "    return delta_3 * a_2.T\n",
    "def compute_gradient_input_to_hidden(delta_2, a_1):\n",
    "    return delta_2 * a_1.T\n",
    "\n",
    "def comp_grad(inpt, true_values, activation1, activation2, eval_f, eval_f_grad, need_extra_grad, activation_gradient1, activation_gradient2):\n",
    "    lay1 = compute_predict_layer1(inpt)\n",
    "    \n",
    "    act1 = compute_activation_layer(activation1, lay1)\n",
    "    lay2 = compute_predict_layer2(act1)\n",
    "    act2 = compute_activation_layer(activation2, lay2)\n",
    "    \n",
    "    #loss = eval_f(act2, true_values)\n",
    "    \n",
    "    delta_3 = eval_f_grad(true_values, act2)\n",
    "    \n",
    "    if need_extra_grad:\n",
    "        delta_3 = activation_gradient2(delta_3)\n",
    "    grad_hidden_output = compute_gradient_hidden_to_output(delta_3, act1)\n",
    "    g_prime_of_z = activation_gradient1(lay1)\n",
    "    delta_2 = gradient_bias_layer1(delta_3, g_prime_of_z)\n",
    "    grad_input_hidden = compute_gradient_input_to_hidden(delta_2, inpt)\n",
    "    \n",
    "   \n",
    "    \n",
    "    return flatten(grad_input_hidden, delta_2, grad_hidden_output, delta_3)\n",
    "    \n",
    "def comp_loss(inpt, true_values, activation1, activation2, eval_f):\n",
    "    lay1 = compute_predict_layer1(inpt)\n",
    "    \n",
    "    act1 = compute_activation_layer(activation1, lay1)\n",
    "    lay2 = compute_predict_layer2(act1)\n",
    "    act2 = compute_activation_layer(activation2, lay2)\n",
    "    \n",
    "    loss = eval_f(act2, true_values)\n",
    "    return loss\n",
    "\n",
    "def grad_check(inpt, true_values, activation1, activation2, eval_f, epsilon):\n",
    "        lay1 = compute_predict_layer1(inpt)\n",
    "        act1 = compute_activation_layer(activation1, lay1)\n",
    "        lay2 = compute_predict_layer2(act1)\n",
    "        act2 = compute_activation_layer(activation2, lay2)\n",
    "        loss_reg = eval_f(true_values, act2)\n",
    "        \n",
    "        def calc_grad(mat):\n",
    "            return_mat = np.matlib.zeros(mat.shape)\n",
    "            for i in range(mat.shape[0]):\n",
    "                \n",
    "                for j in range(mat.shape[1]):\n",
    "                    mat[i, j] += epsilon\n",
    "                    lay1 = compute_predict_layer1(inpt)\n",
    "                    act1 = compute_activation_layer(activation1, lay1)\n",
    "                    lay2 = compute_predict_layer2(act1)\n",
    "                    act2 = compute_activation_layer(activation2, lay2)\n",
    "                    return_mat[i, j] = (eval_f(true_values, act2) - loss_reg) / epsilon\n",
    "                    mat[i, j] -= epsilon\n",
    "            return return_mat\n",
    "        #return calc_grad(input_to_hidden_weight_matrix), calc_grad(input_to_hidden_bias_vector),calc_grad(hidden_to_output_weight_matrix), calc_grad(hidden_to_output_bias_vector)\n",
    "        return calc_grad(input_to_hidden_weight_matrix)\n",
    "#print(comp_grad(training_data[0, :].T, labelp[0, :].T, sigmoid, sigmoid, compute_cost_cross_entropy, gradient_cross_entropy, False, sigmoid_gradient, sigmoid_gradient))\n",
    "\n",
    "\n",
    "#print(grad_check(training_data[0, :].T, labelp[0, :].T, sigmoid, sigmoid, compute_cost_cross_entropy, 0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_gradient(thet, lamb, training_data, true_values, activation1, activation2, eval_f, eval_f_grad, need_extra_grad, activation_gradient1, activation_gradient2):\n",
    "    summ = np.zeros(len(thet))\n",
    "    for i in range(training_data.shape[0]):\n",
    "        summ += comp_grad(training_data[i, :].T, true_value[0, :].T, sigmoid, sigmoid, compute_cost_cross_entropy, gradient_cross_entropy, False, sigmoid_gradient, sigmoid_gradient)\n",
    "    summ += thet * lamb\n",
    "    avg = summ / training_data.shape[0]\n",
    "\n",
    "def comp_total_loss(thet, lamb, training_data, true_values, activation1, activation2, eval_f):\n",
    "    summ = np.zeros(len(thet))\n",
    "    for i in range(training_data.shape[0]):\n",
    "        summ += comp_loss(training_data[i, :].T, true_values[i, :].T, activation1, activation2, eval_f)\n",
    "    summ += sum([s*s for s in thet]) * lamb\n",
    "    avg = summ / training_data.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_theta = flatten(input_to_hidden_weight_matrix, input_to_hidden_bias_vector, hidden_to_output_weight_matrix, hidden_to_output_bias_vector)\n",
    "\n",
    "lamb = 0.01\n",
    "\n",
    "def cost_func_and_gradient(theta, *args):\n",
    "    lamb = args[0]\n",
    "    training_data = args[1]\n",
    "    labelp = args[2]\n",
    "    input_to_hidden_weight_matrix, input_to_hidden_bias_vector, hidden_to_output_weight_matrix, hidden_to_output_bias_vector = unflatten(theta)\n",
    "    thet = flatten(input_to_hidden_weight_matrix, np.matlib.zeros(input_to_hidden_bias_vector.shape), hidden_to_output_weight_matrix, np.matlib.zeros(hidden_to_output_bias_vector.shape))\n",
    "    \n",
    "    def compute_total_loss_gradient(thet, lamb, training_data, true_values, activation1, activation2, eval_f, eval_f_grad, need_extra_grad, activation_gradient1, activation_gradient2):\n",
    "        \n",
    "        summ = np.zeros(len(thet))\n",
    "        sump = 0.0\n",
    "        for i in range(training_data.shape[0]):\n",
    "            k = comp_grad(training_data[i, :].T, true_values[i, :].T, activation1, activation2, eval_f, eval_f_grad, need_extra_grad, activation_gradient1, activation_gradient2)\n",
    "            summ += k[1]\n",
    "            sump += k[0]\n",
    "            \n",
    "        \n",
    "        summ += thet* lamb\n",
    "        \n",
    "        sump += sum([s *s for s in thet]) * lamb\n",
    "        avgp = sump / training_data.shape[0]\n",
    "        avgm = summ / training_data.shape[0]\n",
    "        \n",
    "        return avgp, avgm\n",
    "        \n",
    "    def compute_predict_layer1(input_layer):\n",
    "        lay1 = input_to_hidden_weight_matrix * input_layer + input_to_hidden_bias_vector * np.matrix(np.ones(input_layer.shape[1]))\n",
    "        return lay1\n",
    "    def compute_activation_layer(activation, layer):\n",
    "        return activation(layer)\n",
    "    def compute_predict_layer2(lay1):\n",
    "        lay2 = hidden_to_output_weight_matrix * lay1 + hidden_to_output_bias_vector * np.matrix(np.ones(lay1.shape[1]))\n",
    "        return lay2\n",
    "\n",
    "    def compute_cost_cross_entropy(true_values, predicted_values):\n",
    "        sum = 0\n",
    "\n",
    "        for k in range(true_values.shape[0]):\n",
    "\n",
    "\n",
    "            sum+= np.log(predicted_values[k, 0]) * true_values[k, 0] + (1 - true_values[k, 0]) * np.log(1 - predicted_values[k, 0])\n",
    "\n",
    "\n",
    "        return -1 * sum\n",
    "    def sigmoid(npmat):\n",
    "        \n",
    "        \n",
    "        return 1 / (1 + np.exp(-1 * np.matrix(np.clip(npmat, -500, 500 ))))\n",
    "\n",
    "    def apply_func_to_matrix(s, func):\n",
    "        for i in range(s.shape[0]):\n",
    "            for j in range(s.shape[1]):\n",
    "                s[i, j] = func(s[i, j])\n",
    "        return s\n",
    "\n",
    "    def relu(npmat):\n",
    "        return 0.5 * abs(npmat) + .5* npmat\n",
    "    def compute_cost_mean_squared_error(true_values, predicted_values):\n",
    "        return np.matrix(np.apply_along_axis(sum_of_squares, 1, np.array(true_values - predicted_values)))\n",
    "\n",
    "    def sum_of_squares(arr):\n",
    "        return sum([x*x for x in arr])\n",
    "\n",
    "    def gradient_mean_squared_error(true_values, predicted_values):\n",
    "        return 2* (true_values - predicted_values)\n",
    "\n",
    "\n",
    "\n",
    "    def gradient_cross_entropy(true_values, activation_three):\n",
    "        return  activation_three - true_values\n",
    "\n",
    "    def gradient_bias_layer1(delta_3, activation_gradient_of_z):\n",
    "\n",
    "        return np.multiply(hidden_to_output_weight_matrix.T * delta_3, activation_gradient_of_z)\n",
    "\n",
    "    def relu_gradient(layer):\n",
    "        def relu_grad_one(num):\n",
    "            if num > 0:\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 0.0\n",
    "        return apply_func_to_matrix(layer, relu_grad_one)\n",
    "\n",
    "    def sigmoid_gradient(layer):\n",
    "        def sigmoid_grad_one(num):\n",
    "            return (1 - sigmoid(num)) * sigmoid(num)\n",
    "        return apply_func_to_matrix(layer, sigmoid_grad_one)\n",
    "    def compute_gradient_bias_layer2(predicted_values, activation_three, grad_function):\n",
    "        return grad_function(predicted_values, activation_three)\n",
    "    def compute_gradient_hidden_to_output(delta_3, a_2):\n",
    "        return delta_3 * a_2.T\n",
    "    def compute_gradient_input_to_hidden(delta_2, a_1):\n",
    "        return delta_2 * a_1.T\n",
    "\n",
    "    def comp_grad(inpt, true_values, activation1, activation2, eval_f, eval_f_grad, need_extra_grad, activation_gradient1, activation_gradient2):\n",
    "        lay1 = compute_predict_layer1(inpt)\n",
    "\n",
    "        act1 = compute_activation_layer(activation1, lay1)\n",
    "        lay2 = compute_predict_layer2(act1)\n",
    "        act2 = compute_activation_layer(activation2, lay2)\n",
    "        \n",
    "\n",
    "        loss = eval_f(true_values, act2)\n",
    "        \n",
    "        delta_3 = eval_f_grad(true_values, act2)\n",
    "\n",
    "        if need_extra_grad:\n",
    "            delta_3 = activation_gradient2(delta_3)\n",
    "        grad_hidden_output = compute_gradient_hidden_to_output(delta_3, act1)\n",
    "        g_prime_of_z = activation_gradient1(lay1)\n",
    "        delta_2 = gradient_bias_layer1(delta_3, g_prime_of_z)\n",
    "        grad_input_hidden = compute_gradient_input_to_hidden(delta_2, inpt)\n",
    "        \n",
    "    \n",
    "\n",
    "        return loss, flatten(grad_input_hidden, delta_2, grad_hidden_output, delta_3)\n",
    "    \n",
    "    \n",
    "\n",
    "    return compute_total_loss_gradient(thet, lamb, training_data, labelp, sigmoid, sigmoid, compute_cost_cross_entropy, gradient_cross_entropy, False, sigmoid_gradient, sigmoid_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(theta, *args):\n",
    "    return cost_func_and_gradient(theta, *args)[0]\n",
    "def gradient(theta, *args):\n",
    "    return cost_func_and_gradient(theta, *args)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aagarwal_601/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "1\n",
      "13.0100404544\n",
      "2\n",
      "9.72599054032\n",
      "3\n",
      "8.1051171028\n",
      "4\n",
      "6.09244689758\n",
      "5\n",
      "5.16716885243\n",
      "6\n",
      "4.72728309357\n",
      "7\n",
      "3.98595323359\n",
      "8\n",
      "3.26254856333\n",
      "9\n",
      "2.77586865554\n",
      "10\n",
      "2.6847273137\n",
      "11\n",
      "3.49502626109\n",
      "12\n",
      "2.59308444383\n",
      "13\n",
      "2.51970523237\n",
      "14\n",
      "2.496393956\n",
      "15\n",
      "2.47282076107\n",
      "16\n",
      "2.41334906686\n",
      "17\n",
      "2.77673283859\n",
      "18\n",
      "2.34766048284\n",
      "19\n",
      "2.02364569783\n",
      "20\n",
      "1.63688224749\n",
      "21\n",
      "1.48088459104\n",
      "22\n",
      "2.58443939133\n",
      "23\n",
      "2.03209183893\n",
      "24\n",
      "1.8494824817\n",
      "25\n",
      "1.96371491555\n",
      "26\n",
      "1.94584218339\n",
      "27\n",
      "1.96931667726\n",
      "28\n",
      "1.7050102181\n",
      "29\n",
      "2.2104815332\n",
      "30\n",
      "1.99041615938\n",
      "31\n",
      "2.06345046452\n",
      "32\n",
      "1.39896061202\n",
      "33\n",
      "1.56655038954\n",
      "34\n",
      "1.61731517375\n",
      "35\n",
      "1.71765460843\n",
      "36\n",
      "1.43957398799\n",
      "37\n",
      "1.73687207334\n",
      "38\n",
      "1.68343030909\n",
      "39\n",
      "1.96153997689\n",
      "40\n",
      "1.5232050714\n",
      "41\n",
      "1.78035548276\n",
      "42\n",
      "1.53472564248\n",
      "43\n",
      "1.57339654924\n",
      "44\n",
      "1.32347402132\n",
      "45\n",
      "1.36870698281\n",
      "46\n",
      "1.57521155563\n",
      "47\n",
      "1.20687523594\n",
      "48\n",
      "1.51505877583\n",
      "49\n",
      "1.20410385693\n",
      "50\n",
      "1.19269567556\n",
      "51\n",
      "1.33435173083\n",
      "52\n",
      "1.50727035591\n",
      "53\n",
      "1.14518255799\n",
      "54\n",
      "1.65456782516\n",
      "55\n",
      "1.2412026455\n",
      "56\n",
      "0.997504473693\n",
      "57\n",
      "1.51778227929\n",
      "58\n",
      "0.967092919815\n",
      "59\n",
      "1.53989272859\n",
      "60\n",
      "1.46555137669\n",
      "61\n",
      "1.56776736909\n",
      "62\n",
      "1.44286537579\n",
      "63\n",
      "1.35126671631\n",
      "64\n",
      "1.43555483803\n",
      "65\n",
      "1.40401659192\n",
      "66\n",
      "1.15724055385\n",
      "67\n",
      "1.27317779868\n",
      "68\n",
      "1.13437124998\n",
      "69\n",
      "1.09775383267\n",
      "70\n",
      "1.66016999448\n",
      "71\n",
      "1.02499036553\n",
      "72\n",
      "1.4931556718\n",
      "73\n",
      "1.68935622226\n",
      "74\n",
      "1.67451184977\n",
      "75\n",
      "0.868561972539\n",
      "76\n",
      "1.48946378794\n",
      "77\n",
      "1.43998050616\n",
      "78\n",
      "1.31335496523\n",
      "79\n",
      "1.3858644313\n",
      "80\n",
      "1.17707761558\n",
      "81\n",
      "1.24088303827\n",
      "82\n",
      "1.15803535197\n",
      "83\n",
      "0.949173400209\n",
      "84\n",
      "0.914883773602\n",
      "85\n",
      "0.945787656237\n",
      "86\n",
      "0.989872919042\n",
      "87\n",
      "1.04620105057\n",
      "88\n",
      "0.778355440483\n",
      "89\n",
      "1.13126290742\n",
      "90\n",
      "0.90318327288\n",
      "91\n",
      "1.02458048167\n",
      "92\n",
      "1.18039323506\n",
      "93\n",
      "1.07280070815\n",
      "94\n",
      "1.1688234533\n",
      "95\n",
      "1.05950027164\n",
      "96\n",
      "0.873093811504\n",
      "97\n",
      "1.27397785728\n",
      "98\n",
      "1.09499321951\n",
      "99\n",
      "1.43770203438\n",
      "100\n",
      "0.997261269903\n",
      "101\n",
      "0.926650907092\n",
      "102\n",
      "1.11481608351\n",
      "103\n",
      "1.37559522069\n",
      "104\n",
      "0.899454265978\n",
      "105\n",
      "1.43572672776\n",
      "106\n",
      "1.01304804947\n",
      "107\n",
      "0.929260991624\n",
      "108\n",
      "0.794410317312\n",
      "109\n",
      "1.08435229014\n",
      "110\n",
      "0.774357642995\n",
      "111\n",
      "1.18546958615\n",
      "112\n",
      "1.33730964775\n",
      "113\n",
      "1.10310474785\n",
      "114\n",
      "1.02632219565\n",
      "115\n",
      "1.14112875495\n",
      "116\n",
      "1.29110922824\n",
      "117\n",
      "0.904027356402\n",
      "118\n",
      "0.784716967545\n",
      "119\n",
      "0.570112029942\n",
      "120\n",
      "1.20766347\n",
      "121\n",
      "0.874914145775\n",
      "122\n",
      "0.595900161587\n",
      "123\n",
      "1.29123482507\n",
      "124\n",
      "0.793875263302\n",
      "125\n",
      "0.793586376287\n",
      "126\n",
      "0.755394823947\n",
      "127\n",
      "0.755296750619\n",
      "128\n",
      "0.647002512883\n",
      "129\n",
      "0.975193791754\n",
      "130\n",
      "0.801880675713\n",
      "131\n",
      "0.899025516253\n",
      "132\n",
      "0.871225886213\n",
      "133\n",
      "1.1391268053\n",
      "134\n",
      "0.844170627885\n",
      "135\n",
      "0.899882283304\n",
      "136\n",
      "1.17135930706\n",
      "137\n",
      "0.938030622593\n",
      "138\n",
      "1.03073839348\n",
      "139\n",
      "0.789304579141\n",
      "140\n",
      "1.16104190584\n",
      "141\n",
      "0.773862221578\n",
      "142\n",
      "1.17588751241\n",
      "143\n",
      "0.885739220995\n",
      "144\n",
      "0.688750696619\n",
      "145\n",
      "0.940158140555\n",
      "146\n",
      "0.792852739881\n",
      "147\n",
      "0.961235002233\n",
      "148\n",
      "0.592263812559\n",
      "149\n",
      "1.17031756861\n",
      "150\n",
      "0.62996858059\n",
      "151\n",
      "0.740870320335\n",
      "152\n",
      "0.996815097671\n",
      "153\n",
      "1.14333016085\n",
      "154\n",
      "0.787484761735\n",
      "155\n",
      "0.762645600404\n",
      "156\n",
      "0.763818328767\n",
      "157\n",
      "0.749325967984\n",
      "158\n",
      "1.07759855398\n",
      "159\n",
      "0.990186967271\n",
      "160\n",
      "0.81033749833\n",
      "161\n",
      "0.560003980442\n",
      "162\n",
      "0.809887972038\n",
      "163\n",
      "0.481783812604\n",
      "164\n",
      "1.10547229966\n",
      "165\n",
      "1.13959397919\n",
      "166\n",
      "0.423457735376\n",
      "167\n",
      "0.677441069231\n",
      "168\n",
      "0.76915176947\n",
      "169\n",
      "0.973155965551\n",
      "170\n",
      "0.836296388132\n",
      "171\n",
      "0.987741034595\n",
      "172\n",
      "0.775627319212\n",
      "173\n",
      "0.715824904397\n",
      "174\n",
      "0.818208372003\n",
      "175\n",
      "1.01922018579\n",
      "176\n",
      "1.094798986\n",
      "177\n",
      "0.982114099728\n",
      "178\n",
      "0.553986986976\n",
      "179\n",
      "0.663285993672\n",
      "180\n",
      "0.501570232356\n",
      "181\n",
      "0.840981091064\n",
      "182\n",
      "0.729120789173\n",
      "183\n",
      "1.09931457705\n",
      "184\n",
      "0.750073879543\n",
      "185\n",
      "0.921666712083\n",
      "186\n",
      "0.679538259352\n",
      "187\n",
      "0.668925222974\n",
      "188\n",
      "0.893064490036\n",
      "189\n",
      "0.738751731704\n",
      "190\n",
      "0.542156210056\n",
      "191\n",
      "0.827444382883\n",
      "192\n",
      "0.675414937794\n",
      "193\n",
      "0.555048019789\n",
      "194\n",
      "0.690992156899\n",
      "195\n",
      "0.714810565643\n",
      "196\n",
      "0.809168383325\n",
      "197\n",
      "0.619738623983\n",
      "198\n",
      "0.859351647783\n",
      "199\n",
      "0.468083130333\n",
      "200\n",
      "0.74549856175\n",
      "201\n",
      "0.85079727469\n",
      "202\n",
      "0.717547366316\n",
      "203\n",
      "0.860811501758\n",
      "204\n",
      "0.834985431431\n",
      "205\n",
      "0.808003836596\n",
      "206\n",
      "0.820101035475\n",
      "207\n",
      "0.656567528191\n",
      "208\n",
      "0.800764857143\n",
      "209\n",
      "0.994823419435\n",
      "210\n",
      "0.742840915993\n",
      "211\n",
      "0.629564023846\n",
      "212\n",
      "0.882014176526\n",
      "213\n",
      "0.720334212488\n",
      "214\n",
      "0.64588484193\n",
      "215\n",
      "0.457865194738\n",
      "216\n",
      "0.741801361985\n",
      "217\n",
      "0.582856891214\n",
      "218\n",
      "0.836192074479\n",
      "219\n",
      "0.686000528626\n",
      "220\n",
      "0.768644666027\n",
      "221\n",
      "0.71347263103\n",
      "222\n",
      "0.676832346483\n",
      "223\n",
      "0.860679320497\n",
      "224\n",
      "0.745881022788\n",
      "225\n",
      "0.469543412542\n",
      "226\n",
      "0.702023019502\n",
      "227\n",
      "0.80348785183\n",
      "228\n",
      "0.854293061518\n",
      "229\n",
      "0.931493798524\n",
      "230\n",
      "0.568021511453\n",
      "231\n",
      "0.353643384265\n",
      "232\n",
      "0.705850087371\n",
      "233\n",
      "0.709673826878\n",
      "234\n",
      "0.716166256054\n",
      "235\n",
      "1.06197545978\n",
      "236\n",
      "0.695361943235\n",
      "237\n",
      "0.598702038481\n",
      "238\n",
      "1.20526837303\n",
      "239\n",
      "0.840861023099\n",
      "240\n",
      "0.513208155684\n",
      "241\n",
      "0.808895751046\n",
      "242\n",
      "0.684613392902\n",
      "243\n",
      "0.670429864916\n",
      "244\n",
      "0.66972725686\n",
      "245\n",
      "0.75888117303\n",
      "246\n",
      "1.23766502774\n",
      "247\n",
      "0.609481578054\n",
      "248\n",
      "0.46720708688\n",
      "249\n",
      "0.709104591733\n",
      "250\n",
      "0.536853017847\n",
      "251\n",
      "1.03320908741\n",
      "252\n",
      "0.645578635302\n",
      "253\n",
      "0.92761698711\n",
      "254\n",
      "0.85693016755\n",
      "255\n",
      "0.69662589364\n",
      "256\n",
      "0.530244205859\n",
      "257\n",
      "0.848256645226\n",
      "258\n",
      "0.588517452498\n",
      "259\n",
      "0.631204454609\n",
      "260\n",
      "0.843028699995\n",
      "261\n",
      "0.878159401157\n",
      "262\n",
      "0.622183333739\n",
      "263\n",
      "0.852477510959\n",
      "264\n",
      "0.895429019207\n",
      "265\n",
      "0.963629698093\n",
      "266\n",
      "0.931102378867\n",
      "267\n",
      "0.764848519316\n",
      "268\n",
      "0.684365994645\n",
      "269\n",
      "0.38438457857\n",
      "270\n",
      "0.89182928569\n",
      "271\n",
      "0.585237464327\n",
      "272\n",
      "0.355075366269\n",
      "273\n",
      "0.743012549532\n",
      "274\n",
      "0.686843852524\n",
      "275\n",
      "0.613478470383\n",
      "276\n",
      "0.614523390894\n",
      "277\n",
      "0.6486435484\n",
      "278\n",
      "0.699255113769\n",
      "279\n",
      "0.63754608938\n",
      "280\n",
      "0.66604425184\n",
      "281\n",
      "0.876092258954\n",
      "282\n",
      "0.723208411067\n",
      "283\n",
      "0.651170356692\n",
      "284\n",
      "0.823839004964\n",
      "285\n",
      "0.944709669506\n",
      "286\n",
      "0.774866096351\n",
      "287\n",
      "0.96506146499\n",
      "288\n",
      "0.743262185478\n",
      "289\n",
      "0.566521065713\n",
      "290\n",
      "0.474712976231\n",
      "291\n",
      "0.692322594157\n",
      "292\n",
      "0.802100895705\n",
      "293\n",
      "0.394814732178\n",
      "294\n",
      "0.364325487937\n",
      "295\n",
      "0.595248932541\n",
      "296\n",
      "0.798477696271\n",
      "297\n",
      "0.87554199081\n",
      "298\n",
      "0.859216044597\n",
      "299\n",
      "0.897933987809\n",
      "300\n",
      "0.481779205922\n",
      "301\n",
      "0.743892070725\n",
      "302\n",
      "0.398495209381\n",
      "303\n",
      "0.571398814616\n",
      "304\n",
      "0.890281619658\n",
      "305\n",
      "0.57087614527\n",
      "306\n",
      "0.700538063772\n",
      "307\n",
      "0.849235233592\n",
      "308\n",
      "0.430727042995\n",
      "309\n",
      "0.928553473303\n",
      "310\n",
      "0.464968394839\n",
      "311\n",
      "0.956757686823\n",
      "312\n",
      "0.677809973411\n",
      "313\n",
      "0.513024749032\n",
      "314\n",
      "0.61144242984\n",
      "315\n",
      "0.517204535999\n",
      "316\n",
      "0.541050033167\n",
      "317\n",
      "0.650801549869\n",
      "318\n",
      "0.748268760439\n",
      "319\n",
      "0.89632770427\n",
      "320\n",
      "0.468030350237\n",
      "321\n",
      "0.573545330237\n",
      "322\n",
      "0.720152949058\n",
      "323\n",
      "0.524832297722\n",
      "324\n",
      "0.642619066282\n",
      "325\n",
      "0.732142872607\n",
      "326\n",
      "0.915329385216\n",
      "327\n",
      "0.571255627802\n",
      "328\n",
      "0.639997529379\n",
      "329\n",
      "0.535486628854\n",
      "330\n",
      "0.565487183114\n",
      "331\n",
      "0.460854230085\n",
      "332\n",
      "0.852329434202\n",
      "333\n",
      "0.345072585867\n",
      "334\n",
      "0.346873953173\n",
      "335\n",
      "0.64802279042\n",
      "336\n",
      "0.874831641199\n",
      "337\n",
      "0.500532385716\n",
      "338\n",
      "0.824569341871\n",
      "339\n",
      "1.03752580084\n",
      "340\n",
      "0.608967627799\n",
      "341\n",
      "0.529718969721\n",
      "342\n",
      "0.742778835031\n",
      "343\n",
      "0.56495486955\n",
      "344\n",
      "0.479478164984\n",
      "345\n",
      "0.669161960438\n",
      "346\n",
      "0.438742054417\n",
      "347\n",
      "0.547310191399\n",
      "348\n",
      "0.521797203741\n",
      "349\n",
      "0.463966910571\n",
      "350\n",
      "0.461837018327\n",
      "351\n",
      "0.564424559141\n",
      "352\n",
      "0.467675347741\n",
      "353\n",
      "0.497348342629\n",
      "354\n",
      "0.493140396004\n",
      "355\n",
      "0.386368701396\n",
      "356\n",
      "0.65989046451\n",
      "357\n",
      "0.478843979122\n",
      "358\n",
      "0.498038644564\n",
      "359\n",
      "0.594723272463\n",
      "360\n",
      "0.434128954688\n",
      "361\n",
      "0.64170001155\n",
      "362\n",
      "0.573755394187\n",
      "363\n",
      "0.953444305443\n",
      "364\n",
      "0.536322712552\n",
      "365\n",
      "0.506787499498\n",
      "366\n",
      "0.681306143221\n",
      "367\n",
      "0.659653879602\n",
      "368\n",
      "0.573427205866\n",
      "369\n",
      "0.758643977219\n",
      "370\n",
      "0.450073915033\n",
      "371\n",
      "0.936415944176\n",
      "372\n",
      "0.619186325445\n",
      "373\n",
      "0.8374229411\n",
      "374\n",
      "0.415989557068\n",
      "375\n",
      "0.464562781153\n",
      "376\n",
      "0.590597606776\n",
      "377\n",
      "0.506096604482\n",
      "378\n",
      "0.648320689729\n",
      "379\n",
      "0.504118622621\n",
      "380\n",
      "0.61323731126\n",
      "381\n",
      "0.476753744748\n",
      "382\n",
      "0.523125917996\n",
      "383\n",
      "0.681752844311\n",
      "384\n",
      "0.651673695505\n",
      "385\n",
      "0.50523937087\n",
      "386\n",
      "0.812643567091\n",
      "387\n",
      "0.339556745106\n",
      "388\n",
      "0.50606966317\n",
      "389\n",
      "0.780663041938\n",
      "390\n",
      "0.775028922041\n",
      "391\n",
      "0.78984104207\n",
      "392\n",
      "0.646039942573\n",
      "393\n",
      "0.543470878076\n",
      "394\n",
      "0.536602378481\n",
      "395\n",
      "0.732017237222\n",
      "396\n",
      "0.798096092972\n",
      "397\n",
      "0.45264965536\n",
      "398\n",
      "0.412775079474\n",
      "399\n",
      "0.430715076969\n",
      "400\n",
      "0.454976392857\n",
      "401\n",
      "0.485546031729\n",
      "402\n",
      "0.499209011265\n",
      "403\n",
      "0.702383224299\n",
      "404\n",
      "0.35092553679\n",
      "405\n",
      "0.571179089603\n",
      "406\n",
      "0.370173876179\n",
      "407\n",
      "0.590234337801\n",
      "408\n",
      "0.712395601004\n",
      "409\n",
      "0.539042790892\n",
      "410\n",
      "0.670129259249\n",
      "411\n",
      "0.450227688261\n",
      "412\n",
      "0.528450898063\n",
      "413\n",
      "0.871371854058\n",
      "414\n",
      "0.391984357553\n",
      "415\n",
      "1.09277922009\n",
      "416\n",
      "0.510502839153\n",
      "417\n",
      "0.56544856539\n",
      "418\n",
      "0.455804166226\n",
      "419\n",
      "0.402414468914\n",
      "420\n",
      "0.762759772594\n",
      "421\n",
      "0.365826894302\n",
      "422\n",
      "0.647003017011\n",
      "423\n",
      "0.465208411146\n",
      "424\n",
      "0.594291991036\n",
      "425\n",
      "0.599166899951\n",
      "426\n",
      "0.690103905991\n",
      "427\n",
      "0.471601959089\n",
      "428\n",
      "0.312606495108\n",
      "429\n",
      "0.643750705331\n",
      "430\n",
      "0.449430838415\n",
      "431\n",
      "0.581166545103\n",
      "432\n",
      "0.437619877564\n",
      "433\n",
      "0.423419587461\n",
      "434\n",
      "0.58016723364\n",
      "435\n",
      "0.64183260365\n",
      "436\n",
      "0.610756392498\n",
      "437\n",
      "0.745681046604\n",
      "438\n",
      "0.552020235882\n",
      "439\n",
      "0.706195154048\n",
      "440\n",
      "0.300624312644\n",
      "441\n",
      "0.60533391908\n",
      "442\n",
      "0.890545479576\n",
      "443\n",
      "0.289741178305\n",
      "444\n",
      "0.680677020915\n",
      "445\n",
      "0.61772852833\n",
      "446\n",
      "0.487832561037\n",
      "447\n",
      "0.439179748838\n",
      "448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21248846333\n",
      "449\n",
      "0.428763949399\n",
      "450\n",
      "0.637209140354\n",
      "451\n",
      "0.646399377029\n",
      "452\n",
      "0.705149765179\n",
      "453\n",
      "0.650690546299\n",
      "454\n",
      "0.640911544227\n",
      "455\n",
      "0.469870401647\n",
      "456\n",
      "0.660585512939\n",
      "457\n",
      "0.374509577013\n",
      "458\n",
      "0.594998169659\n",
      "459\n",
      "0.439443261789\n",
      "460\n",
      "0.726710283317\n",
      "461\n",
      "0.545145031487\n",
      "462\n",
      "0.865210060736\n",
      "463\n",
      "0.531107083723\n",
      "464\n",
      "0.527909758579\n",
      "465\n",
      "0.565040329506\n",
      "466\n",
      "0.526290300147\n",
      "467\n",
      "0.642908609982\n",
      "468\n",
      "0.532452518361\n",
      "469\n",
      "0.616378778436\n",
      "470\n",
      "0.515900844466\n",
      "471\n",
      "0.419604087747\n",
      "472\n",
      "0.504476541378\n",
      "473\n",
      "0.362379334643\n",
      "474\n",
      "0.520418809903\n",
      "475\n",
      "0.555976521342\n",
      "476\n",
      "0.534777590121\n",
      "477\n",
      "0.340671367191\n",
      "478\n",
      "0.415393094223\n",
      "479\n",
      "0.485692969248\n",
      "480\n",
      "0.47817908224\n",
      "481\n",
      "0.466213984426\n",
      "482\n",
      "0.414291719637\n",
      "483\n",
      "0.574100826576\n",
      "484\n",
      "0.450934002797\n",
      "485\n",
      "0.616424885277\n",
      "486\n",
      "0.628538544697\n",
      "487\n",
      "0.245602487306\n",
      "488\n",
      "0.654924698307\n",
      "489\n",
      "0.783291635906\n",
      "490\n",
      "0.445318676389\n",
      "491\n",
      "0.564970418868\n",
      "492\n",
      "0.591225523001\n",
      "493\n",
      "0.620193508318\n",
      "494\n",
      "0.260232983958\n",
      "495\n",
      "0.254087955106\n",
      "496\n",
      "0.720719337732\n",
      "497\n",
      "0.57302155288\n",
      "498\n",
      "0.515645445536\n",
      "499\n",
      "0.723811838188\n",
      "500\n",
      "0.34156804876\n",
      "501\n",
      "0.419286910956\n",
      "502\n",
      "0.548621076966\n",
      "503\n",
      "0.497738074486\n",
      "504\n",
      "0.445607208482\n",
      "505\n",
      "0.545709396149\n",
      "506\n",
      "0.709717795219\n",
      "507\n",
      "0.691695298331\n",
      "508\n",
      "0.531260372329\n",
      "509\n",
      "0.324500931524\n",
      "510\n",
      "0.459736437971\n",
      "511\n",
      "0.512121742932\n",
      "512\n",
      "0.593063894\n",
      "513\n",
      "0.467020154252\n",
      "514\n",
      "0.686826732608\n",
      "515\n",
      "0.602095716005\n",
      "516\n",
      "0.414408037711\n",
      "517\n",
      "0.608227149797\n",
      "518\n",
      "0.523478246461\n",
      "519\n",
      "0.550971323285\n",
      "520\n",
      "0.372597642482\n",
      "521\n",
      "0.374107248562\n",
      "522\n",
      "0.513544070434\n",
      "523\n",
      "0.473839166533\n",
      "524\n",
      "0.440656656205\n",
      "525\n",
      "0.512951377977\n",
      "526\n",
      "0.431551031187\n",
      "527\n",
      "0.380402829907\n",
      "528\n",
      "0.633678539949\n",
      "529\n",
      "0.49900789362\n",
      "530\n",
      "0.927889965303\n",
      "531\n",
      "0.634013191915\n",
      "532\n",
      "0.567554001528\n",
      "533\n",
      "0.389704194547\n",
      "534\n",
      "0.362391909746\n",
      "535\n",
      "0.463913437649\n",
      "536\n",
      "0.430749671309\n",
      "537\n",
      "0.58551136866\n",
      "538\n",
      "0.43886780232\n",
      "539\n",
      "0.720341855287\n",
      "540\n",
      "0.521185210491\n",
      "541\n",
      "0.469274945282\n",
      "542\n",
      "0.545814715528\n",
      "543\n",
      "0.445230327323\n",
      "544\n",
      "0.612874705265\n",
      "545\n",
      "0.487052449448\n",
      "546\n",
      "0.383145548687\n",
      "547\n",
      "0.580767831582\n",
      "548\n",
      "0.651250217919\n",
      "549\n",
      "0.337909748262\n",
      "550\n",
      "0.709772148722\n",
      "551\n",
      "0.593197288165\n",
      "552\n",
      "0.629792723173\n",
      "553\n",
      "0.58904480677\n",
      "554\n",
      "0.481354207656\n",
      "555\n",
      "0.636262425174\n",
      "556\n",
      "0.35860067161\n",
      "557\n",
      "0.523569314121\n",
      "558\n",
      "0.710465994527\n",
      "559\n",
      "0.58584582674\n",
      "560\n",
      "0.47620579532\n",
      "561\n",
      "0.463932776164\n",
      "562\n",
      "0.813965423399\n",
      "563\n",
      "0.484775413061\n",
      "564\n",
      "0.447433383909\n",
      "565\n",
      "0.572142974413\n",
      "566\n",
      "0.386712495638\n",
      "567\n",
      "0.271607080564\n",
      "568\n",
      "0.524773665436\n",
      "569\n",
      "0.416054185892\n",
      "570\n",
      "0.465243688531\n",
      "571\n",
      "0.387877334907\n",
      "572\n",
      "0.381185195062\n",
      "573\n",
      "0.631476776553\n",
      "574\n",
      "0.271124355626\n",
      "575\n",
      "0.522560386121\n",
      "576\n",
      "0.595505162202\n",
      "577\n",
      "0.617417893623\n",
      "578\n",
      "0.56164983274\n",
      "579\n",
      "0.438182553196\n",
      "580\n",
      "0.66358899829\n",
      "581\n",
      "0.501135145721\n",
      "582\n",
      "0.341937579348\n",
      "583\n",
      "0.385380301449\n",
      "584\n",
      "0.606983797727\n",
      "585\n",
      "0.441789218971\n",
      "586\n",
      "0.287155123846\n",
      "587\n",
      "0.459320560116\n",
      "588\n",
      "0.553393669054\n",
      "589\n",
      "0.4111884229\n",
      "590\n",
      "0.391909877093\n",
      "591\n",
      "0.543344134855\n",
      "592\n",
      "0.492717940035\n",
      "593\n",
      "0.466580426671\n",
      "594\n",
      "0.586754869544\n",
      "595\n",
      "0.410527529908\n",
      "596\n",
      "0.304240622971\n",
      "597\n",
      "0.658506182244\n",
      "598\n",
      "0.409587449775\n",
      "599\n",
      "0.292460981447\n",
      "600\n",
      "0.486035753476\n",
      "601\n",
      "0.509126665184\n",
      "602\n",
      "0.466717120132\n",
      "603\n",
      "0.443250099927\n",
      "604\n",
      "0.329431823486\n",
      "605\n",
      "0.293961158727\n",
      "606\n",
      "0.487802124331\n",
      "607\n",
      "0.7238214507\n",
      "608\n",
      "0.357810524293\n",
      "609\n",
      "0.50080942494\n",
      "610\n",
      "0.368961212464\n",
      "611\n",
      "0.419356280265\n",
      "612\n",
      "0.567621440397\n",
      "613\n",
      "0.466049562448\n",
      "614\n",
      "0.791184262088\n",
      "615\n",
      "0.47530282994\n",
      "616\n",
      "0.510027406729\n",
      "617\n",
      "0.481250016172\n",
      "618\n",
      "0.370011156263\n",
      "619\n",
      "0.385135601284\n",
      "620\n",
      "0.381246943273\n",
      "621\n",
      "0.397553998536\n",
      "622\n",
      "0.753232897479\n",
      "623\n",
      "0.667875731771\n",
      "624\n",
      "0.735202677855\n",
      "625\n",
      "0.864675416884\n",
      "626\n",
      "0.483021280394\n",
      "627\n",
      "0.3382030889\n",
      "628\n",
      "0.266991824795\n",
      "629\n",
      "0.329235486059\n",
      "630\n",
      "0.635157958608\n",
      "631\n",
      "0.416053207014\n",
      "632\n",
      "0.373864885268\n",
      "633\n",
      "0.470216587918\n",
      "634\n",
      "0.450793558693\n",
      "635\n",
      "0.587746921928\n",
      "636\n",
      "0.330482502092\n",
      "637\n",
      "0.305528127008\n",
      "638\n",
      "0.302651361456\n",
      "639\n",
      "0.390740515972\n",
      "640\n",
      "0.34756635588\n",
      "641\n",
      "0.253488207666\n",
      "642\n",
      "0.419239909853\n",
      "643\n",
      "0.621193992518\n",
      "644\n",
      "0.375701207039\n",
      "645\n",
      "0.508474002898\n",
      "646\n",
      "0.783001472598\n",
      "647\n",
      "0.609066286137\n",
      "648\n",
      "0.658456560943\n",
      "649\n",
      "0.531374054809\n",
      "650\n",
      "0.499920435645\n",
      "651\n",
      "0.37067195348\n",
      "652\n",
      "0.285876410238\n",
      "653\n",
      "0.326060816832\n",
      "654\n",
      "0.525062941899\n",
      "655\n",
      "0.341841910223\n",
      "656\n",
      "0.481918034991\n",
      "657\n",
      "0.470300635514\n",
      "658\n",
      "0.333410578127\n",
      "659\n",
      "0.524904163206\n",
      "660\n",
      "0.375276848312\n",
      "661\n",
      "0.626749027397\n",
      "662\n",
      "0.491095292374\n",
      "663\n",
      "0.716822724176\n",
      "664\n",
      "0.327852508026\n",
      "665\n",
      "0.314859073751\n",
      "666\n",
      "0.50114999607\n",
      "667\n",
      "0.423545984985\n",
      "668\n",
      "0.382444379891\n",
      "669\n",
      "0.448541860034\n",
      "670\n",
      "0.374986951418\n",
      "671\n",
      "0.428011415605\n",
      "672\n",
      "0.473602760153\n",
      "673\n",
      "0.591870254678\n",
      "674\n",
      "0.710180103676\n",
      "675\n",
      "0.609214108566\n",
      "676\n",
      "0.549116141082\n",
      "677\n",
      "0.535266118426\n",
      "678\n",
      "0.430610355497\n",
      "679\n",
      "0.398938481042\n",
      "680\n",
      "0.732348458879\n",
      "681\n",
      "0.668838968096\n",
      "682\n",
      "0.379949037242\n",
      "683\n",
      "0.600720973167\n",
      "684\n",
      "0.449212159735\n",
      "685\n",
      "0.492815152119\n",
      "686\n",
      "0.365235669244\n",
      "687\n",
      "0.466966362794\n",
      "688\n",
      "0.478239401212\n",
      "689\n",
      "0.403695827733\n",
      "690\n",
      "0.39095045788\n",
      "691\n",
      "0.377667518625\n",
      "692\n",
      "0.427107611828\n",
      "693\n",
      "0.573077373358\n",
      "694\n",
      "0.467112594971\n",
      "695\n",
      "0.249727576335\n",
      "696\n",
      "0.472693565228\n",
      "697\n",
      "0.496468763205\n",
      "698\n",
      "0.425497470966\n",
      "699\n",
      "0.700697928046\n",
      "700\n",
      "0.449809378911\n",
      "701\n",
      "0.540984130577\n",
      "702\n",
      "0.42196042453\n",
      "703\n",
      "0.414294300873\n",
      "704\n",
      "0.656579419103\n",
      "705\n",
      "0.479985746454\n",
      "706\n",
      "0.459435137869\n",
      "707\n",
      "0.262325329398\n",
      "708\n",
      "0.322593302652\n",
      "709\n",
      "0.400502352361\n",
      "710\n",
      "0.583924402154\n",
      "711\n",
      "0.580575781254\n",
      "712\n",
      "0.371411046894\n",
      "713\n",
      "0.404109555473\n",
      "714\n",
      "0.693334010925\n",
      "715\n",
      "0.580199991943\n",
      "716\n",
      "0.556084408326\n",
      "717\n",
      "0.445229761878\n",
      "718\n",
      "0.41030425317\n",
      "719\n",
      "0.564454336879\n",
      "720\n",
      "0.534074864907\n",
      "721\n",
      "0.636805717806\n",
      "722\n",
      "0.37399523754\n",
      "723\n",
      "0.381807134257\n",
      "724\n",
      "0.693239100256\n",
      "725\n",
      "0.456867398311\n",
      "726\n",
      "0.453970364949\n",
      "727\n",
      "0.28991369805\n",
      "728\n",
      "0.778459413658\n",
      "729\n",
      "0.438871199714\n",
      "730\n",
      "0.4437033531\n",
      "731\n",
      "0.463083478021\n",
      "732\n",
      "0.59411587519\n",
      "733\n",
      "0.441674288777\n",
      "734\n",
      "0.272568963433\n",
      "735\n",
      "0.648277698781\n",
      "736\n",
      "0.226498558117\n",
      "737\n",
      "0.298419018643\n",
      "738\n",
      "0.596298532332\n",
      "739\n",
      "0.624943774749\n",
      "740\n",
      "0.516868033853\n",
      "741\n",
      "0.444963731864\n",
      "742\n",
      "0.673588504582\n",
      "743\n",
      "0.452077517014\n",
      "744\n",
      "0.340028639801\n",
      "745\n",
      "0.185757517559\n",
      "746\n",
      "0.290496593162\n",
      "747\n",
      "0.396258346424\n",
      "748\n",
      "0.438075895873\n",
      "749\n",
      "0.229072330165\n",
      "750\n",
      "0.375536469546\n",
      "751\n",
      "0.427871177544\n",
      "752\n",
      "0.32928383112\n",
      "753\n",
      "0.27634377922\n",
      "754\n",
      "0.69518472301\n",
      "755\n",
      "0.367596118089\n",
      "756\n",
      "0.488027349601\n",
      "757\n",
      "0.522027834916\n",
      "758\n",
      "0.505784766221\n",
      "759\n",
      "0.554841135012\n",
      "760\n",
      "0.535146839227\n",
      "761\n",
      "0.437376692323\n",
      "762\n",
      "0.281524682698\n",
      "763\n",
      "0.31872224326\n",
      "764\n",
      "0.249770039615\n",
      "765\n",
      "0.436608780087\n",
      "766\n",
      "0.508511237474\n",
      "767\n",
      "0.351798946033\n",
      "768\n",
      "0.392277412853\n",
      "769\n",
      "0.43923493416\n",
      "770\n",
      "0.369829530819\n",
      "771\n",
      "0.611622827999\n",
      "772\n",
      "0.391847010775\n",
      "773\n",
      "0.403655684234\n",
      "774\n",
      "0.529222018932\n",
      "775\n",
      "0.388966316845\n",
      "776\n",
      "0.439652670338\n",
      "777\n",
      "0.762192154541\n",
      "778\n",
      "0.558142371555\n",
      "779\n",
      "0.369671040695\n",
      "780\n",
      "0.60975457629\n",
      "781\n",
      "0.464149234318\n",
      "782\n",
      "0.639286646954\n",
      "783\n",
      "0.49571769302\n",
      "784\n",
      "0.361548434743\n",
      "785\n",
      "0.457147243162\n",
      "786\n",
      "0.451081604655\n",
      "787\n",
      "0.6938976683\n",
      "788\n",
      "0.496295363125\n",
      "789\n",
      "0.432422346206\n",
      "790\n",
      "0.331181253401\n",
      "791\n",
      "0.475519345993\n",
      "792\n",
      "0.518392959327\n",
      "793\n",
      "0.56082508473\n",
      "794\n",
      "0.372583185942\n",
      "795\n",
      "0.416824318985\n",
      "796\n",
      "0.351834239538\n",
      "797\n",
      "0.673082573933\n",
      "798\n",
      "0.322063008157\n",
      "799\n",
      "0.417878102837\n",
      "800\n",
      "0.843406624521\n",
      "801\n",
      "0.64421426787\n",
      "802\n",
      "0.359116516455\n",
      "803\n",
      "0.510608564365\n",
      "804\n",
      "0.432617639963\n",
      "805\n",
      "0.396329232317\n",
      "806\n",
      "0.36058309858\n",
      "807\n",
      "0.448937889175\n",
      "808\n",
      "0.379384237988\n",
      "809\n",
      "0.351530176047\n",
      "810\n",
      "0.45147864383\n",
      "811\n",
      "0.348596309928\n",
      "812\n",
      "0.644450002002\n",
      "813\n",
      "0.226405649541\n",
      "814\n",
      "0.255607065385\n",
      "815\n",
      "0.428391478078\n",
      "816\n",
      "0.616914279625\n",
      "817\n",
      "0.503421817053\n",
      "818\n",
      "0.620223694784\n",
      "819\n",
      "0.283900664917\n",
      "820\n",
      "0.407483883893\n",
      "821\n",
      "0.251204768002\n",
      "822\n",
      "0.349320554221\n",
      "823\n",
      "0.538670757394\n",
      "824\n",
      "0.496509920452\n",
      "825\n",
      "0.461474442085\n",
      "826\n",
      "0.460211651531\n",
      "827\n",
      "0.709982123418\n",
      "828\n",
      "0.431401565719\n",
      "829\n",
      "0.621991928589\n",
      "830\n",
      "0.357450697915\n",
      "831\n",
      "0.54042743018\n",
      "832\n",
      "0.396579333592\n",
      "833\n",
      "0.348175442834\n",
      "834\n",
      "0.350635817507\n",
      "835\n",
      "0.443671630063\n",
      "836\n",
      "0.598744143331\n",
      "837\n",
      "0.419325557325\n",
      "838\n",
      "0.612080239928\n",
      "839\n",
      "0.34969214969\n",
      "840\n",
      "0.231530251845\n",
      "841\n",
      "0.528113087126\n",
      "842\n",
      "0.558776570178\n",
      "843\n",
      "0.400633872506\n",
      "844\n",
      "0.432429071288\n",
      "845\n",
      "0.383275013621\n",
      "846\n",
      "0.421847759195\n",
      "847\n",
      "0.454754253529\n",
      "848\n",
      "0.529621975954\n",
      "849\n",
      "0.442509585038\n",
      "850\n",
      "0.213627716618\n",
      "851\n",
      "0.446151333685\n",
      "852\n",
      "0.323461828642\n",
      "853\n",
      "0.632735278538\n",
      "854\n",
      "0.695475203961\n",
      "855\n",
      "0.406853646671\n",
      "856\n",
      "0.40774212653\n",
      "857\n",
      "0.433025452119\n",
      "858\n",
      "0.292426917647\n",
      "859\n",
      "0.391868385836\n",
      "860\n",
      "0.305647042496\n",
      "861\n",
      "0.525821356364\n",
      "862\n",
      "0.387429655657\n",
      "863\n",
      "0.438474781393\n",
      "864\n",
      "0.452057799563\n",
      "865\n",
      "0.31557027953\n",
      "866\n",
      "0.513143183234\n",
      "867\n",
      "0.358964269881\n",
      "868\n",
      "0.341518980208\n",
      "869\n",
      "0.301574066088\n",
      "870\n",
      "0.68528302598\n",
      "871\n",
      "0.304766754235\n",
      "872\n",
      "0.354918849704\n",
      "873\n",
      "0.493019256383\n",
      "874\n",
      "0.522539234452\n",
      "875\n",
      "0.446970050335\n",
      "876\n",
      "0.38361924795\n",
      "877\n",
      "0.649173473692\n",
      "878\n",
      "0.497612388636\n",
      "879\n",
      "0.28003150345\n",
      "880\n",
      "0.341863701155\n",
      "881\n",
      "0.559335035631\n",
      "882\n",
      "0.355523897202\n",
      "883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.656784638698\n",
      "884\n",
      "0.593392806991\n",
      "885\n",
      "0.518875487114\n",
      "886\n",
      "0.602738102101\n",
      "887\n",
      "0.310785630445\n",
      "888\n",
      "0.294734015635\n",
      "889\n",
      "0.581090318924\n",
      "890\n",
      "0.600885701455\n",
      "891\n",
      "0.489522742429\n",
      "892\n",
      "0.669314650209\n",
      "893\n",
      "0.348454728547\n",
      "894\n",
      "0.334767629161\n",
      "895\n",
      "0.264424400321\n",
      "896\n",
      "0.264899004711\n",
      "897\n",
      "0.444539944238\n",
      "898\n",
      "0.2376747738\n",
      "899\n",
      "0.37797453992\n",
      "900\n",
      "0.228190944634\n",
      "901\n",
      "0.253752135767\n",
      "902\n",
      "0.473434001958\n",
      "903\n",
      "0.273818296243\n",
      "904\n",
      "0.515568945304\n",
      "905\n",
      "0.212931403892\n",
      "906\n",
      "0.343544233472\n",
      "907\n",
      "0.362270237396\n",
      "908\n",
      "0.478052148423\n",
      "909\n",
      "0.322121368224\n",
      "910\n",
      "0.23208410145\n",
      "911\n",
      "0.31611554105\n",
      "912\n",
      "0.472930833783\n",
      "913\n",
      "0.459110326966\n",
      "914\n",
      "0.450747992016\n",
      "915\n",
      "0.527775564975\n",
      "916\n",
      "0.268488126133\n",
      "917\n",
      "0.476736389423\n",
      "918\n",
      "0.621581224235\n",
      "919\n",
      "0.240345937774\n",
      "920\n",
      "0.223786706179\n",
      "921\n",
      "0.379887183618\n",
      "922\n",
      "0.309126492555\n",
      "923\n",
      "0.345195672622\n",
      "924\n",
      "0.316496668467\n",
      "925\n",
      "0.422899235776\n",
      "926\n",
      "0.298337882394\n",
      "927\n",
      "0.385742209154\n",
      "928\n",
      "0.373878885616\n",
      "929\n",
      "0.373590973103\n",
      "930\n",
      "0.319918229519\n",
      "931\n",
      "0.237352528991\n",
      "932\n",
      "0.355871963387\n",
      "933\n",
      "0.560382546766\n",
      "934\n",
      "0.580872565689\n",
      "935\n",
      "0.53214522406\n",
      "936\n",
      "0.34123707363\n",
      "937\n",
      "0.329615295241\n",
      "938\n",
      "0.233555508412\n",
      "939\n",
      "0.378548033694\n",
      "940\n",
      "0.320932456473\n",
      "941\n",
      "0.390612721942\n",
      "942\n",
      "0.417689810066\n",
      "943\n",
      "0.330825104237\n",
      "944\n",
      "0.370146817025\n",
      "945\n",
      "0.424273939642\n",
      "946\n",
      "0.3943020563\n",
      "947\n",
      "0.248216591546\n",
      "948\n",
      "0.339530497429\n",
      "949\n",
      "0.51363607317\n",
      "950\n",
      "0.46533362509\n",
      "951\n",
      "0.246780157472\n",
      "952\n",
      "0.342451549273\n",
      "953\n",
      "0.466821688002\n",
      "954\n",
      "0.424942942357\n",
      "955\n",
      "0.463502292185\n",
      "956\n",
      "0.417918094176\n",
      "957\n",
      "0.50448941939\n",
      "958\n",
      "0.298499588035\n",
      "959\n",
      "0.412312822782\n",
      "960\n",
      "0.299981831552\n",
      "961\n",
      "0.367477984586\n",
      "962\n",
      "0.304443076396\n",
      "963\n",
      "0.529497202951\n",
      "964\n",
      "0.637805296971\n",
      "965\n",
      "0.510363242308\n",
      "966\n",
      "0.45145177807\n",
      "967\n",
      "0.415733421735\n",
      "968\n",
      "0.360385919103\n",
      "969\n",
      "0.279903984219\n",
      "970\n",
      "0.362779753623\n",
      "971\n",
      "0.537812949669\n",
      "972\n",
      "0.585604784997\n",
      "973\n",
      "0.454220123377\n",
      "974\n",
      "0.335318016786\n",
      "975\n",
      "0.508185368626\n",
      "976\n",
      "0.383702285444\n",
      "977\n",
      "0.309305633428\n",
      "978\n",
      "0.367847359777\n",
      "979\n",
      "0.276184226606\n",
      "980\n",
      "0.447837457111\n",
      "981\n",
      "0.24948552844\n",
      "982\n",
      "0.351065073133\n",
      "983\n",
      "0.449670392308\n",
      "984\n",
      "0.342747307947\n",
      "985\n",
      "0.441073377917\n",
      "986\n",
      "0.456420690178\n",
      "987\n",
      "0.214875703078\n",
      "988\n",
      "0.273754250896\n",
      "989\n",
      "0.311493300522\n",
      "990\n",
      "0.409465821318\n",
      "991\n",
      "0.865206458979\n",
      "992\n",
      "0.600200099614\n",
      "993\n",
      "0.307571289377\n",
      "994\n",
      "0.446074516695\n",
      "995\n",
      "0.504595737469\n",
      "996\n",
      "0.589771552927\n",
      "997\n",
      "0.386363169515\n",
      "998\n",
      "0.316699762121\n",
      "999\n",
      "0.36794071221\n",
      "1000\n",
      "0.347576393423\n",
      "1001\n",
      "0.50087543645\n",
      "1002\n",
      "0.294573291352\n",
      "1003\n",
      "0.350926666674\n",
      "1004\n",
      "0.242216517567\n",
      "1005\n",
      "0.552280288308\n",
      "1006\n",
      "0.423530365551\n",
      "1007\n",
      "0.414125704842\n",
      "1008\n",
      "0.20362821643\n",
      "1009\n",
      "0.416488848103\n",
      "1010\n",
      "0.499697393156\n",
      "1011\n",
      "0.38507667243\n",
      "1012\n",
      "0.370182113414\n",
      "1013\n",
      "0.287532472046\n",
      "1014\n",
      "0.361873569678\n",
      "1015\n",
      "0.535645686345\n",
      "1016\n",
      "0.256672223241\n",
      "1017\n",
      "0.369870966205\n",
      "1018\n",
      "0.518090916684\n",
      "1019\n",
      "0.493905547851\n",
      "1020\n",
      "0.423485278626\n",
      "1021\n",
      "0.549474855967\n",
      "1022\n",
      "0.157755707879\n",
      "1023\n",
      "0.366922997826\n",
      "1024\n",
      "0.264958560077\n",
      "1025\n",
      "0.504683302812\n",
      "1026\n",
      "0.3631961196\n",
      "1027\n",
      "0.308474265033\n",
      "1028\n",
      "0.144270550568\n",
      "1029\n",
      "0.270196117011\n",
      "1030\n",
      "0.311827457466\n",
      "1031\n",
      "0.31287097958\n",
      "1032\n",
      "0.175867889107\n",
      "1033\n",
      "0.259789267163\n",
      "1034\n",
      "0.234754149719\n",
      "1035\n",
      "0.26874745097\n",
      "1036\n",
      "0.349272965073\n",
      "1037\n",
      "0.604193069448\n",
      "1038\n",
      "0.346760396709\n",
      "1039\n",
      "0.314340515413\n",
      "1040\n",
      "0.430932349211\n",
      "1041\n",
      "0.3365424669\n",
      "1042\n",
      "0.588022827894\n",
      "1043\n",
      "0.25522749357\n",
      "1044\n",
      "0.449548837001\n",
      "1045\n",
      "0.237368283106\n",
      "1046\n",
      "0.272643549604\n",
      "1047\n",
      "0.183340499709\n",
      "1048\n",
      "0.41495074882\n",
      "1049\n",
      "0.353886161092\n",
      "1050\n",
      "0.389894024984\n",
      "1051\n",
      "0.372176586891\n",
      "1052\n",
      "0.330280740858\n",
      "1053\n",
      "0.504583592117\n",
      "1054\n",
      "0.386266096717\n",
      "1055\n",
      "0.278287403372\n",
      "1056\n",
      "0.319226806843\n",
      "1057\n",
      "0.438075545614\n",
      "1058\n",
      "0.640305380634\n",
      "1059\n",
      "0.474549338667\n",
      "1060\n",
      "0.394473424084\n",
      "1061\n",
      "0.373212198707\n",
      "1062\n",
      "0.299873385473\n",
      "1063\n",
      "0.425240019072\n",
      "1064\n",
      "0.523855651743\n",
      "1065\n",
      "0.393808137301\n",
      "1066\n",
      "0.51059207879\n",
      "1067\n",
      "0.27358615401\n",
      "1068\n",
      "0.502400522351\n",
      "1069\n",
      "0.322355302674\n",
      "1070\n",
      "0.409512997174\n",
      "1071\n",
      "0.40643604964\n",
      "1072\n",
      "0.417000354443\n",
      "1073\n",
      "0.288383997382\n",
      "1074\n",
      "0.452895522592\n",
      "1075\n",
      "0.403468228938\n",
      "1076\n",
      "0.462533093346\n",
      "1077\n",
      "0.356703362868\n",
      "1078\n",
      "0.344093575775\n",
      "1079\n",
      "0.295956195981\n",
      "1080\n",
      "0.434740244242\n",
      "1081\n",
      "0.3753821717\n",
      "1082\n",
      "0.346042640693\n",
      "1083\n",
      "0.139986407467\n",
      "1084\n",
      "0.197187625944\n",
      "1085\n",
      "0.414266581442\n",
      "1086\n",
      "0.404056749734\n",
      "1087\n",
      "0.34438691392\n",
      "1088\n",
      "0.268562868755\n",
      "1089\n",
      "0.463995885368\n",
      "1090\n",
      "0.532179892933\n",
      "1091\n",
      "0.51845467358\n",
      "1092\n",
      "0.410985704965\n",
      "1093\n",
      "0.245425799601\n",
      "1094\n",
      "0.414756820397\n",
      "1095\n",
      "0.464084725574\n",
      "1096\n",
      "0.516888282939\n",
      "1097\n",
      "0.306292517929\n",
      "1098\n",
      "0.376198604236\n",
      "1099\n",
      "0.398895274086\n",
      "1100\n",
      "0.329757138318\n",
      "1101\n",
      "0.277252537496\n",
      "1102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-411-8401f1e8885e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_func_and_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mcost_func_and_gradient\u001b[0;34m(theta, *args)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompute_total_loss_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_cost_cross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_cross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mcompute_total_loss_gradient\u001b[0;34m(thet, lamb, training_data, true_values, activation1, activation2, eval_f, eval_f_grad, need_extra_grad, activation_gradient1, activation_gradient2)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msump\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_f_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_extra_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_gradient1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_gradient2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0msumm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0msump\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mcomp_grad\u001b[0;34m(inpt, true_values, activation1, activation2, eval_f, eval_f_grad, need_extra_grad, activation_gradient1, activation_gradient2)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mdelta_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_gradient2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mgrad_hidden_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient_hidden_to_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mg_prime_of_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_gradient1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlay1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mdelta_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_bias_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_prime_of_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mgrad_input_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient_input_to_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mrelu_gradient\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_func_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_grad_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigmoid_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mapply_func_to_matrix\u001b[0;34m(s, func)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mrelu_grad_one\u001b[0;34m(num)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrelu_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrelu_grad_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha = 0.00005\n",
    "A = np.hstack((training_data, labels))\n",
    "theta = init_theta\n",
    "for _ in range(10000):\n",
    "    print(_)\n",
    "    batch_xs_ys = A[np.random.choice(A.shape[0], 100), :]\n",
    "    batch_xs = batch_xs_ys[:, :-1]\n",
    "    batch_ys = batch_xs_ys[:, -1]\n",
    "    z = np.matlib.zeros((len(batch_ys), size))\n",
    "    for i in range(len(batch_ys)):\n",
    "        z[i, batch_ys[i, 0]] = 1\n",
    "    val = cost_func_and_gradient(theta, lamb, batch_xs, z)\n",
    "    print(val[0])\n",
    "    theta = theta - alpha * val[1]\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00368629  0.00761133 -0.00047078 ..., -0.06005936 -0.07524761\n",
      "  0.05390293]\n",
      "0\n",
      "0.465940606015\n",
      "1\n",
      "0.387664089795\n",
      "2\n",
      "0.441308110128\n",
      "3\n",
      "0.439668818181\n",
      "4\n",
      "0.486661022894\n",
      "5\n",
      "0.491912921692\n",
      "6\n",
      "0.323566495962\n",
      "7\n",
      "0.41881225394\n",
      "8\n",
      "0.331358491856\n",
      "9\n",
      "0.397990630301\n",
      "10\n",
      "0.312310050858\n",
      "11\n",
      "0.315671343361\n",
      "12\n",
      "0.369147066735\n",
      "13\n",
      "0.353938629031\n",
      "14\n",
      "0.304863731217\n",
      "15\n",
      "0.396332582117\n",
      "16\n",
      "0.47118573097\n",
      "17\n",
      "0.458247674932\n",
      "18\n",
      "0.386886847819\n",
      "19\n",
      "0.55067950471\n",
      "20\n",
      "0.506839825492\n",
      "21\n",
      "0.220919994159\n",
      "22\n",
      "0.363225114109\n",
      "23\n",
      "0.226437375684\n",
      "24\n",
      "0.293232829788\n",
      "25\n",
      "0.279269851365\n",
      "26\n",
      "0.4495251184\n",
      "27\n",
      "0.314260806567\n",
      "28\n",
      "0.234427602216\n",
      "29\n",
      "0.376297956125\n",
      "30\n",
      "0.941673930211\n",
      "31\n",
      "0.347529090828\n",
      "32\n",
      "0.534894850224\n",
      "33\n",
      "0.45862134441\n",
      "34\n",
      "0.283162940252\n",
      "35\n",
      "0.201043529336\n",
      "36\n",
      "0.327732720899\n",
      "37\n",
      "0.350479033202\n",
      "38\n",
      "0.494399647822\n",
      "39\n",
      "0.412465146066\n",
      "40\n",
      "0.58664813928\n",
      "41\n",
      "0.231195455563\n",
      "42\n",
      "0.334652322454\n",
      "43\n",
      "0.356190230416\n",
      "44\n",
      "0.521661612982\n",
      "45\n",
      "0.369718944387\n",
      "46\n",
      "0.452884465578\n",
      "47\n",
      "0.361523508878\n",
      "48\n",
      "0.433770229873\n",
      "49\n",
      "0.249694152154\n",
      "50\n",
      "0.606542517138\n",
      "51\n",
      "0.284240726107\n",
      "52\n",
      "0.270922853254\n",
      "53\n",
      "0.482960551826\n",
      "54\n",
      "0.422463111717\n",
      "55\n",
      "0.572443966239\n",
      "56\n",
      "0.233471495977\n",
      "57\n",
      "0.402917089903\n",
      "58\n",
      "0.489842744737\n",
      "59\n",
      "0.446885308618\n",
      "60\n",
      "0.546839770855\n",
      "61\n",
      "0.297221898406\n",
      "62\n",
      "0.395938894306\n",
      "63\n",
      "0.340451997181\n",
      "64\n",
      "0.40419364303\n",
      "65\n",
      "0.48817893068\n",
      "66\n",
      "0.297740093871\n",
      "67\n",
      "0.393889734672\n",
      "68\n",
      "0.581998312455\n",
      "69\n",
      "0.288537732649\n",
      "70\n",
      "0.532680641977\n",
      "71\n",
      "0.306414084733\n",
      "72\n",
      "0.199592826747\n",
      "73\n",
      "0.319175922321\n",
      "74\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-412-cd260a9620ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_func_and_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mcost_func_and_gradient\u001b[0;34m(theta, *args)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompute_total_loss_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_cost_cross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_cross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mcompute_total_loss_gradient\u001b[0;34m(thet, lamb, training_data, true_values, activation1, activation2, eval_f, eval_f_grad, need_extra_grad, activation_gradient1, activation_gradient2)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msump\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_f_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_extra_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_gradient1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_gradient2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0msumm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0msump\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mcomp_grad\u001b[0;34m(inpt, true_values, activation1, activation2, eval_f, eval_f_grad, need_extra_grad, activation_gradient1, activation_gradient2)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mdelta_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_gradient2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mgrad_hidden_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient_hidden_to_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mg_prime_of_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_gradient1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlay1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mdelta_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_bias_layer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_prime_of_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mgrad_input_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient_input_to_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mrelu_gradient\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_func_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_grad_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigmoid_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-409-5c068ed0f86e>\u001b[0m in \u001b[0;36mapply_func_to_matrix\u001b[0;34m(s, func)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(theta)\n",
    "alpha = 0.000005\n",
    "A = np.hstack((training_data, labels))\n",
    "for _ in range(10000):\n",
    "    print(_)\n",
    "    batch_xs_ys = A[np.random.choice(A.shape[0], 100), :]\n",
    "    batch_xs = batch_xs_ys[:, :-1]\n",
    "    batch_ys = batch_xs_ys[:, -1]\n",
    "    z = np.matlib.zeros((len(batch_ys), size))\n",
    "    for i in range(len(batch_ys)):\n",
    "        z[i, batch_ys[i, 0]] = 1\n",
    "    val = cost_func_and_gradient(theta, lamb, batch_xs, z)\n",
    "    print(val[0])\n",
    "    theta = theta - alpha * val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00368629  0.00761133 -0.00047078 ..., -0.06005942 -0.07524809\n",
      "  0.05390216]\n",
      "0.4722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4722"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important = theta\n",
    "print(theta)\n",
    "\n",
    "def test(theta, data, labels):\n",
    "    input_to_hidden_weight_matrix, input_to_hidden_bias_vector, hidden_to_output_weight_matrix, hidden_to_output_bias_vector = unflatten(theta)\n",
    "    \n",
    "    def compute_predict_layer1(input_layer):\n",
    "        lay1 = input_to_hidden_weight_matrix * input_layer + input_to_hidden_bias_vector * np.matrix(np.ones(input_layer.shape[1]))\n",
    "        return lay1\n",
    "    def compute_activation_layer(activation, layer):\n",
    "        return activation(layer)\n",
    "    def compute_predict_layer2(lay1):\n",
    "        lay2 = hidden_to_output_weight_matrix * lay1 + hidden_to_output_bias_vector * np.matrix(np.ones(lay1.shape[1]))\n",
    "        return lay2\n",
    "    def sigmoid(npmat):\n",
    "        \n",
    "        \n",
    "        return 1 / (1 + np.exp(-1 * np.matrix(np.clip(npmat, -500, 500 ))))\n",
    "    accuracy = 0\n",
    "    total = data.shape[0]\n",
    "    for i in range(data.shape[0]):\n",
    "        lay1 = compute_predict_layer1(data[i, :].T)\n",
    "        act1 = compute_activation_layer(sigmoid, lay1)\n",
    "        lay2 = compute_predict_layer2(act1)\n",
    "        final = np.array(compute_activation_layer(sigmoid, lay2).T)[0]\n",
    "       \n",
    "        if np.argmax(final) == labels[i, 0]:\n",
    "        \n",
    "            accuracy +=1\n",
    "        \n",
    "    print(accuracy / total)\n",
    "    return accuracy / total     \n",
    "\n",
    "test(theta, testing_data, testing_data_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00461464  0.00746492 -0.0069617  ..., -0.03016109 -0.10424411\n",
      " -0.12719614]\n",
      "0.9295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9295"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important = theta\n",
    "print(theta)\n",
    "\n",
    "def test(theta, data, labels):\n",
    "    input_to_hidden_weight_matrix, input_to_hidden_bias_vector, hidden_to_output_weight_matrix, hidden_to_output_bias_vector = unflatten(theta)\n",
    "    \n",
    "    def compute_predict_layer1(input_layer):\n",
    "        lay1 = input_to_hidden_weight_matrix * input_layer + input_to_hidden_bias_vector * np.matrix(np.ones(input_layer.shape[1]))\n",
    "        return lay1\n",
    "    def compute_activation_layer(activation, layer):\n",
    "        return activation(layer)\n",
    "    def compute_predict_layer2(lay1):\n",
    "        lay2 = hidden_to_output_weight_matrix * lay1 + hidden_to_output_bias_vector * np.matrix(np.ones(lay1.shape[1]))\n",
    "        return lay2\n",
    "    def sigmoid(npmat):\n",
    "        \n",
    "        \n",
    "        return 1 / (1 + np.exp(-1 * np.matrix(np.clip(npmat, -500, 500 ))))\n",
    "    accuracy = 0\n",
    "    total = data.shape[0]\n",
    "    for i in range(data.shape[0]):\n",
    "        lay1 = compute_predict_layer1(data[i, :].T)\n",
    "        act1 = compute_activation_layer(sigmoid, lay1)\n",
    "        lay2 = compute_predict_layer2(act1)\n",
    "        final = np.array(compute_activation_layer(sigmoid, lay2).T)[0]\n",
    "       \n",
    "        if np.argmax(final) == labels[i, 0]:\n",
    "        \n",
    "            accuracy +=1\n",
    "        \n",
    "    print(accuracy / total)\n",
    "    return accuracy / total     \n",
    "\n",
    "test(theta, testing_data, testing_data_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#THIS IS THE ACCURACY FOR THE MNIST DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
